% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/statistics.R
\name{sig_model_correctness}
\alias{sig_model_correctness}
\title{Quantify model correctness for signature deconvolution}
\usage{
sig_model_correctness(observed, truth, all_signatures = NULL, validate = TRUE)
}
\arguments{
\item{observed}{Numeric named vector. The fitted model (signature weights). Names must be signature IDs; values are fractional contributions (typically sum to 1). See \code{\link[sigshared:model]{sigshared::example_model()}}.}

\item{truth}{Numeric named vector. The true signature contributions; same format as \code{observed}.}

\item{all_signatures}{Optional character vector. The complete set of possible signature IDs to be evaluated. If \code{NULL}, inferred as the union of names from \code{observed} and \code{truth}.}

\item{validate}{Logical. If \code{TRUE} (default), input vectors are checked for correct formatting, and expanded/reordered as needed. If \code{FALSE}, user must ensure \code{observed} and \code{truth} are already aligned and complete.}
}
\value{
Named list of metrics quantifying model correctness (see Details).
}
\description{
Computes a suite of evaluation metrics for mutational signature fitting. This function compares an "observed" model (from signature fitting) to the "truth" (the true underlying signature contributions) and reports standard accuracy metrics.
}
\details{
The function returns a named list with:
\describe{
\item{fitting_error}{Sum of absolute differences between observed and truth, divided by 2 (range: 0â€“1).}
\item{RMSE}{Root mean squared error between observed and truth.}
\item{n_false_positives}{Number of signatures called present in observed but not present in truth.}
\item{n_false_negatives}{Number of signatures present in truth but missed in observed.}
\item{n_true_positives}{Number of signatures correctly called as present.}
\item{n_true_negatives}{Number of signatures correctly called as absent.}
\item{total_false_positive_contributions}{Sum of weights assigned to false positive signatures.}
\item{precision}{Proportion of detected signatures that are truly present (TP / (TP + FP)).}
\item{recall}{Proportion of true signatures that are detected (TP / (TP + FN)).}
\item{specificity}{Proportion of truly absent signatures that are not detected (TN / (TN + FP)).}
\item{mathews_correlation_coeff}{Matthews Correlation Coefficient (MCC), a balanced measure even for imbalanced classes.}
\item{f1}{F1 score, the harmonic mean of precision and recall.}
\item{balanced_accuracy}{Average of recall and specificity.}
}

For metric calculation, the observed and truth vectors are \strong{expanded and reordered} (if needed) to include all signatures in \code{all_signatures}. Any signatures not present in \code{observed} or \code{truth} are assumed to have zero contribution.

Presence/absence for each signature is defined as "present" if weight > 0, and "absent" if weight == 0.

For each signature, possible outcomes are:
\describe{
\item{True Positive (TP)}{Observed > 0 and Truth > 0 (signature fitted and truly present).}
\item{False Positive (FP)}{Observed > 0 and Truth == 0 (signature fitted but not truly present).}
\item{False Negative (FN)}{Observed == 0 and Truth > 0 (signature missed by fitting but truly present).}
\item{True Negative (TN)}{Observed == 0 and Truth == 0 (signature correctly called absent).}
}

Counts of TP, FP, TN, and FN are used for all classification metrics. Division-by-zero results in \code{NA} for undefined metrics.

Precision, recall, F1, and MCC may not be comparable across datasets with different class balances.
}
\examples{
observed <- c(SBS1 = 0.7, SBS5 = 0.3, SBS18 = 0)
truth <- c(SBS1 = 0.6, SBS5 = 0.4, SBS18 = 0)
sig_model_correctness(observed, truth)

}
\seealso{
\code{\link[sigshared:model]{sigshared::example_model()}}
}
